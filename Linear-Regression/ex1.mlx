data = load('ex1data1.txt'); % load the dataset from the data file into the variables X and y
X = data(:,1); y = data(:,2);

plotData(X,y); % call the function plotData(x,y) in plotData.m

m = length(X); % number of training examples 
X = [ones(m,1), data(:,1)]; % add a column of ones to X
theta = zeros(2,1); % initialize fitting parameters
iterations = 1500;
alpha = 0.01;

% computing the cost J(theta), test
compueCost(X,y,theta);
computeCost(X,y,[-1;2]);

theta = gradientDescent(X,y,theta,alpha,iterations);

fprintf('Theta computed from gradient descen: \n%f, \n%f', theta(1), theta(2))

% plot the linear fit
hold onl %keep previous plot visible 
plot( X(:,2), X*theta, '-')
legend('Training data', 'Linear regression')
hold off; %don't overlay any more plots on this figure 

% predict values for population sizes of 35,000 and 70,000
predict1 = [1, 3.5] * theta;
predict2 = [1, 7] * theta ;
fprintf('For population = 35,000, we predict a profit of %f\n', predict1 * 10000);
fprintf('For population = 70,000, we predict a profit of %f\n', predict2 * 10000);

% visualizing J(theta_0, theta_1):
theta0_vals = linspace(-10, 10, 100) % linspace(x1, x2, n) generates n points. The spacing between the poins is (x2 - x1)/(n - 1)
theta1_vals = linspace(-1, 4, 100);

% initialize J_vals to a matrix of 0's
J_vals = zeros(length(theta0_vals), length(theta1_vals));

% fill out J_vals
for i = 1:length(theta0_vals)
    for j = 1:length(theta1_vals)
        t = [theta0_vals(i); theta1_vals(j)];
        J_vals(i,j) = computeCost(X, y, t);
    end
end

% because of the way meshgrids work in the surf comman, we need to 
% transpose J_vals before calling surf, or else the axes will be flipped
J_vals = J_vals';

% surface plot
figure;
surf(theta0_vals, theta1_vals, J_vals);
xlabel('\theta_0');
ylabel('\theta_1');

% contour plot
figure;
% plot J_vals as 15 contours spaced logarithmically between 0.01 and 100
contour(theta0_vals, theta1_vals, J_vals, logspace(-2, 3, 20)) % logspace(a,b,n) generates n points between decades 10^a and 10^b
xlabel('\theta_0'); 
ylabel('\theta_1');
hold on;
plot(theta(1), theta(2), 'rx', 'MarkerSize', 10, 'LineWidth', 2);
hold off; 

% linear regression with multiple variables 
% load data
data = load('ex1data2.txt');
X = data(:, 1:2);
y = data(:, 3);
m = length(y);

% print out some data points
% first 10 examples from the dataset
fprintf( ' x = [%.0f %.0f], y = %.0f \n', [X(1:10,:) y(1:10,:)]');

% scale features and set them to zero mean
[X, mu, sigma] = featureNormalize(X);

% add intercept term to X
X = [ones(m,1) X];

% run gradient descent and choose some alpha value
alpha = 0.1;
num_iters = 400; 
theta = zeros(3,1);
[theta, ~] = gradientDescentMulti(X, y, theta, alpha, num_iters);

% display gradient descent's result
fprintf('Theta computed from gradient descent: \n%f\n%f\n%f', theta(1), theta(2), theta(3))

% selecting learning rates by choose some alpha values
alpha = 0.1;
num_iters = 100;
theta = zeros(3, 1);
[~, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters);

% plot the convergence graph
plot(1:num_iters, J_history, 'b', 'LineWidth', 2);
xlabel('Number of iterations');
ylabel('Cost J');
hold on;

% choose some alpha like: 0.3, 0.03, 0.01... then plot and compare

% normal equations
data = csvread('ex1data2.txt');
X = data(:, 1:2);
y = data(:, 3);
m = length(y);

X = [ones(m,1) X];
theta = normalEqn(X, y);
fprintf('Theta computed from the normal equations: \n%f\n%f\n%f', theta(1), theta(2), theta(3));

end


